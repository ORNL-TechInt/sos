Examining the lineage of bytes for weapons system data
======================================================
:latex-use-bibliography-environment:
:latex-use-running-title-headings:

Description
-----------
[NOTE]
This is a draft document - not intended as a deliverable or distribution at
this point. The goal is to support the model discussion and to capture issues
raised during the discussions.

source Brad Settlemyer
1.  It must be possible to locate chunks of data or entire files based
on similarities between the provenance data (for example, data that
originated from the same internet service providers).
2.  It must be possible for the user to enumerate provenance data (e.g.
Get a listing of all the originating node names for a file or set of files).
3.  It must be possible for the user to retrieve files simply by
specifying the provenance data of interest.

source Gary Grider
1.  I am wondering what all provenance data would be kept.  Source location 
is helpful, but levels of software that generated it and other things
like that have been asked for in the past.
2.  I think the lineage part comes from understanding what output was
generated from what stack of software.  If someone discovered that there was a
flaw in some stack at some point, and decisions were made on that output that
might be suspect, one could figure out how wide spread that issue might be.
So it really wasn't just check-pointing, more decision support.  It had to do
with weapons certification, but seems to have general uses I would think.  Not
sure about DoD, but a similar concept would seem to apply. 

Actors
------
1.  The object storage system
2.  An object storage provenance query tool
3.  A weapons system visualization application
4.  Kermit, a frustrated, under-appreciated visualization engineer

Assumptions
-----------
The object storage system is filled with a 500TB data set that describes the
performance of a weapon system.  The weapon system data is collected at
multiple times and therefore was gathered on multiple systems (with differing
hardware and software stacks).  The automatic provenance collection system
used with a popular open-source nuclear weapons software has annotated all 
generated data with the hardware information (model numbers, revisions, 
firmware releases, etc.), the software stack used (kernel version, OS version, 
driver versions, compiler version, scheduler version, file system version, 
etc.).

Steps
-----------
1. Kermit learns that the latest Gigtel processor used in his new
  heterogeneous architecture Pray supercomputer may have a defect in the floating
  point hardware, and may be generating incorrect results in a popular
  open-source weapons software package.  Kermit intends to use visualization
  techniques to examine the impact of the floating point problem on the new
  supercomputer. 
2. Kermit uses an object storage provenance query tool to examine the
  data sets generated by the open-source weapons software described in step 1.
  2.1  Kermit starts the query tool and provides a specialized keyword that
    identifies all data stored by the weapons application.
  2.2  The tool displays a list of the provenance fields available for the
    data sets.
  2.3  Kermit indicates that he wishes to enumerate the supercomputer
    platforms for all available datasets
  2.4  Kermit chooses a previous version of a Pray computer that has produced 
    experimentally verified data, and writes down the identifier for that data
    set in his notebook (identifier A).
  2.5  Kermit then chooses a data set generated by the recent Pray computer
    and the tool displays the data set identifier (identifier B) which he also
    writes down in his notebook.
3. Kermit starts the weapons system visualization software with an archived
  data set described with "identifier A" (instance A) and invokes another
  application instance  with the dataset described as "identifier B" (instance
  B).
  3.1  Instance A requests chunks of data from its data set, and begins an
    elapsed time video of the weapon performance and the processors used to
    generate each data region.
  3.2  Instance B requests chunks of data from its data set, and begins an
    elapsed time video of the weapon performance and the processors used to
    generate each data region.
  3.3  Kermit notes that the results are generally comparable, however small
    grainy imperfections exist for dataset B.  Further investigation
    determines that the weapons software generated the data using both GPUs
    and the faulty processors.  Equations solved via GPU on the Pray's
    heterogeneous architecture are comparable to results seen in instance A,
    but results generated on the faulty CPUs are more variable, indicating
    that the data stored for "identifier B" is not an accurate result.
4. Kermit publishes his results at a leading supercomputer conference hosted
  in a cold, overly rainy northwestern location.
5.  Gigtel sends Kermit a cease and desist letter, and Gigtel's CEO invites
  Kermits's manager to a nice dinner at a local steakhouse.
6. Kermit is fired.  It's not easy being green.
   

Variations
----------
1. TBD.


Issues
------
1.  One of the key things here is that the data has to be annotated multiple
ways.  Even from the same computer, only some parts of the data may have been
generated with faulty data (parts that relied on the CPU floating point
behavior rather than the GPU floating point behavior) are spoiled.
2.  Users need to be able to customize the annotation options.  Its too hard
to predict what annotations will be needed in the future.  This leads to
questions about whether archival systems will be compatible with the object
storage annotation scheme.  Maybe that's just in the serialization format 
though.
3.  Two large data sets are in flight at the same time in this use case, so
there are also opportunities for optimization here.
