Importing the CSM Data into an analysis cluster
================================
:latex-use-bibliography-environment:
:latex-use-running-title-headings:

Description
-----------
[NOTE]
This is a draft document - not intended as a deliverable or distribution at
this point. The goal is to support the model discussion and to capture issues
raised during the discussions.

source James Horey
Verbal description of how the CSM data was imported

Actors
------
1.  A HDFS storage system
2.  A user performing the import

Assumptions
-----------
The CSM data is delivered as a series of 600GB EBCDIC encoded files stored in
mainframe's proprietary DB format.  There were probably roughly 10 of these
files to begin with.  The files in general contained database tables, but the
tables were not necessarily complete, and many columns within the tables are
sparsely populated.

Steps
-----------
1.  James develops a process to convert the EBCDIC encoded data into simpler
CSV files, and processes each of the files.  Size grows from 600GB per file to
700GB per file.
2.  James imports the resulting CSV files into an HDFS cluster using the
standard partitioning tools with the resulign data stored in a column-based
storage format called an RCFile with LZO compression resulting in 150-300GB
data files.  The total RCFile data is 12TB in size.  The data is stored in
triplicate, with a cluster size of 24 nodes.

Variations
----------
1.  The RCFile data can be replicated further to enable analysis on up to 47
nodes.

Issues
------
1.  


